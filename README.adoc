= Apache Pulsar Release Candidate Validation Scripts
:toc: macro
:toc-title: Table of Contents
:toclevels: 3

toc::[]

[[overview]]
Scripts to link:https://pulsar.apache.org/contribute/validate-release-candidate/[validate Apache Pulsar release candidates] using Docker containers.
These scripts support both Unix-like systems (Bash) and Windows (PowerShell).

[[prerequisites]]
== Prerequisites

* Docker with docker-in-docker support for running the validation script that launches a Cassandra container inside a container
** This is required for the validation script to work
** You can test docker-in-docker support with this command:
+
[source,shell]
----
docker run --privileged -v /var/run/docker.sock:/var/run/docker.sock --rm -it lhotari/pulsar-release-validation:1 docker ps
----
* Bash (for Unix-like systems) or PowerShell 7+ (for Windows)
* Fast internet connection for downloading the validation docker image and Pulsar release
** The link:https://hub.docker.com/r/lhotari/pulsar-release-validation/tags[validation docker image (≈2.5GB)] includes a snapshot of the majority of the maven dependencies required to build Pulsar.
** The Pulsar build will download the remaining dependencies (≈0.7GB) at build time.

[[usage]]
== Usage

[[repo-setup]]
=== Clone or download the repository

[source,shell]
----
git clone https://github.com/lhotari/pulsar-release-validation
cd pulsar-release-validation
----

or link:https://github.com/lhotari/pulsar-release-validation/archive/refs/heads/master.zip[download the repository as a zip file] and extract it.

[[docker-validation]]
=== Run the validation script in a Docker container

[[unix-validation]]
==== On Unix-like systems (Linux, macOS)

[source,shell]
----
./scripts/validate_pulsar_release_in_docker.sh [release-version] [candidate-number] | tee [log-file-name]
----

Examples:

[source,shell]
----
# Validate release candidate 1 of version 3.0.10
./scripts/validate_pulsar_release_in_docker.sh 3.0.10 1 | tee validate_pulsar_release_`date +%Y-%m-%d_%H-%M-%S`.log

# Validate release candidate 2 of version 3.3.5
./scripts/validate_pulsar_release_in_docker.sh 3.3.5 2 | tee validate_pulsar_release_`date +%Y-%m-%d_%H-%M-%S`.log

# Validate release candidate 2 of version 4.0.3
./scripts/validate_pulsar_release_in_docker.sh 4.0.3 2 | tee validate_pulsar_release_`date +%Y-%m-%d_%H-%M-%S`.log
----

[[windows-validation]]
==== On Windows (PowerShell)

[source,powershell]
----
.\scripts\validate_pulsar_release_in_docker.ps1 [release-version] [candidate-number] | Tee-Object -FilePath [log-file-name]
----

Examples:

[source,powershell]
----
# Validate release candidate 1 of version 3.0.10
.\scripts\validate_pulsar_release_in_docker.ps1 3.0.10 1 | Tee-Object -FilePath "validate_pulsar_release_$(Get-Date -Format 'yyyy-MM-dd_HH-mm-ss').log"

# Validate release candidate 2 of version 3.3.5
.\scripts\validate_pulsar_release_in_docker.ps1 3.3.5 2 | Tee-Object -FilePath "validate_pulsar_release_$(Get-Date -Format 'yyyy-MM-dd_HH-mm-ss').log"

# Validate release candidate 2 of version 4.0.3
.\scripts\validate_pulsar_release_in_docker.ps1 4.0.3 2 | Tee-Object -FilePath "validate_pulsar_release_$(Get-Date -Format 'yyyy-MM-dd_HH-mm-ss').log"
----

[[maven-cache]]
=== Maven Repository Cache

The validation script will use a persistent Docker volume to hold a Maven repository cache to speed up the build process of subsequent release candidate validations.
The cache volume is named `pulsar_release_validation_m2_cache`. It is created automatically when the first release candidate validation is run.
The volume's contents are primed with the maven dependencies included in the validation docker image. However, since it doesn't include all dependencies, the cache volume solution effectively prevents the "downloading the internet" problem when validating release candidates.

If you'd like to delete the cache volume, you can do so with the following command:

[source,shell]
----
docker volume rm pulsar_release_validation_m2_cache
----

To verify the disk usage of the cache volume, you can run the following command:

[source,shell]
----
docker system df -v | grep pulsar_release_validation_m2_cache
----

To get a shell into a container with the cache volume mounted, you can run the following command:

[source,shell]
----
docker run --rm -it -v pulsar_release_validation_m2_cache:/root/.m2 lhotari/pulsar-release-validation-base /bin/bash
----

In the container shell, you can then inspect the contents of the cache volume:

[source,shell]
----
find /root/.m2/repository
----

To disable the Maven repository cache, set the following environment variable:

[source,shell]
----
export PULSAR_RELEASE_VALIDATION_M2_CACHE_VOLUME=none
----

[[alternative-validation]]
== Alternative Ways to Run the Validation Script

[[direct-validation]]
=== Run the validation script directly

One benefit of running the script directly is that if validation fails, you can retry without needing to re-download and rebuild the Pulsar release.

[source,shell]
----
./scripts/validate_pulsar_release.sh [release-version] [candidate-number] | tee [log-file-name]
----

Examples:

[source,shell]
----
# Validate release candidate 1 of version 3.0.10
./scripts/validate_pulsar_release.sh 3.0.10 1 | tee validate_pulsar_release_`date +%Y-%m-%d_%H-%M-%S`.log

# Validate release candidate 2 of version 3.3.5
./scripts/validate_pulsar_release.sh 3.3.5 2 | tee validate_pulsar_release_`date +%Y-%m-%d_%H-%M-%S`.log

# Validate release candidate 2 of version 4.0.3
./scripts/validate_pulsar_release.sh 4.0.3 2 | tee validate_pulsar_release_`date +%Y-%m-%d_%H-%M-%S`.log
----

[[cloud-vm]]
== Cloud VM Instructions

Running the validation in a cloud VM can be an efficient approach, especially for users with limited local resources or bandwidth.

[[vm-requirements]]
=== General Cloud VM Requirements

Debian or Ubuntu based cloud VMs are available from all major cloud providers (AWS, Azure, GCP, etc.).

Pick a VM with at least:

* 8GB of RAM
* 4 CPU cores / 8 virtual CPUs
* 30GB of disk space (choose larger size for better performance)

Release validation for a single release candidate takes about 15 minutes when running on an `e2-highcpu-8` VM in GCP with 200GB of pd-ssd disk space. With low-resource VMs, the validation can take a very long time. That's why it's recommended to run the validation in a cloud VM with sufficient resources. The VM can be deleted after the validation is complete, so the cost is minimal for a single validation run. The VM disk can be snapshotted to retain the configuration and Maven cache for subsequent validation runs. Keeping the disk snapshot will cost less than $1 per month on GCP (depending on the snapshot size).

The instructions below provide specifics for GCP, but similar approaches can be used on AWS and Azure with their respective CLI tools and VM offerings.

[[gcp-vm-creation]]
=== Creating a VM in GCP

On GCP, `e2-highcpu-8` with 200GB of pd-ssd disk space is a good choice for running the validation script (about $0.24 hourly rate).
The 200GB disk space is recommended due to better disk I/O performance of larger disks. After running the validation script, you can stop the VM, create a snapshot of the VM's disk to retain the configuration and Maven cache before deleting the VM and the disk. Keeping the disk snapshot costs less than $1 per month.
There are more details in the <<cost-considerations,cost considerations section>>.

[[gcp-signup]]
==== Signing up for GCP

If you don't have a GCP account, you can sign up for a free trial at link:https://console.cloud.google.com/freetrial[Google Cloud Console].
After signing up, you can create a project to get started. You will need to link:https://cloud.google.com/billing/docs/how-to/verify-billing-enabled#confirm_billing_is_enabled_on_a_project[enable billing] to use the GCP CLI.

[[gcp-cli-setup]]
==== Setting up GCP CLI

If you don't have the GCP CLI (`gcloud`) installed, you can install it with the following command:

[source,shell]
----
curl https://sdk.cloud.google.com | bash
----

There are alternative ways to install the GCP CLI, see link:https://cloud.google.com/sdk/docs/install[the official documentation] for more details.

Login and Select Project:

[source,shell]
----
gcloud auth login
gcloud projects list
gcloud config set project [project-id]
----

Set a default zone to avoid specifying it in every command:

[source,shell]
----
gcloud config set compute/zone us-central1-c
----

You can list the available zones with the following command:

[source,shell]
----
gcloud compute zones list
----


[[new-vm-creation]]
==== Creating a new VM

Please see the <<cost-considerations,cost considerations section>> for details on the VM cost.

[source,shell]
----
gcloud compute instances create pulsar-release-validation \
  --machine-type=e2-highcpu-8 \
  --image-project=ubuntu-os-cloud \
  --image-family=ubuntu-2204-lts \
  --boot-disk-size=200GB \
  --boot-disk-type=pd-ssd
----

[NOTE]
====
If you'd instead like to use a different Ubuntu version, you can run the following command to see which image family options are available:

[source,shell]
----
gcloud compute images list --project=ubuntu-os-cloud --no-standard-images
----
====

Connect to the VM via SSH:

You might need to wait for about 30 seconds for the VM to boot up after the creation command completes. Retry if the connection fails.

[source,shell]
----
gcloud compute ssh pulsar-release-validation
----

[[vm-setup]]
=== VM Setup and Configuration

[[docker-install]]
==== Installing Docker & tooling

This configures the VM optimized for running Java applications, docker containers, and also enables profiling with async-profiler.

[source,shell]
----
# Configures Debian or Ubuntu VM optimized for development testing and running Java applications, docker containers, 
# and also tunes the Linux kernel settings for profiling with async-profiler.

# Install Docker and other tooling
sudo bash <<'EOF'
# Setup options for non-interactive apt-get
export DEBIAN_FRONTEND=noninteractive
APT_OPTIONS="-y -o Dpkg::Options::=--force-confdef -o Dpkg::Options::=--force-confold"
export PERL_BADLANG=0

# Update package index
apt-get update
# Upgrade existing packages to the latest version
apt-get ${APT_OPTIONS} dist-upgrade

# apt packages to install
PACKAGES=$(cat <<'PKGLIST'
  # system utilities 
  sysfsutils locales ca-certificates apt-transport-https snapd
  # docker
  docker.io
  # dev tools, etc.
  python3-minimal git tig vim tmux less ripgrep tree pv fzf
  # monitoring tools
  htop procps sysstat iotop iftop
  # networking tools
  curl wget netcat-openbsd dnsutils iputils-ping
  # compression & encryption utilities
  zip unzip gpg
  # json, xml utilities
  jq xmlstarlet
PKGLIST
)

# Install packages
apt-get ${APT_OPTIONS} install $(echo "$PACKAGES" | grep -v '^\s*#' | tr -s ' ' | tr '\n' ' ')

# Install yq using snap, a yaml tool (which is not available in apt on Ubuntu 22.04)
snap install yq

# Add the current user to the docker group
adduser $(logname) docker
EOF

# User tooling
# install uv, a fast modern package manager for Python for handling Python scripts that could be added later
curl -LsSf https://astral.sh/uv/install.sh | sh

# Tune Linux Transparent HugePages (THP) for Java processes in a persistent way with sysfsutils
cat <<EOF | sudo tee /etc/sysfs.d/transparent_hugepage.conf
# use "madvise" Linux Transparent HugePages (THP) setting
# https://www.kernel.org/doc/html/latest/admin-guide/mm/transhuge.html
# "madvise" is generally a better option than the default "always" setting
# Based on Azul instructions from https://docs.azul.com/prime/Enable-Huge-Pages#transparent-huge-pages-thp
kernel/mm/transparent_hugepage/enabled=madvise
kernel/mm/transparent_hugepage/shmem_enabled=madvise
kernel/mm/transparent_hugepage/defrag=defer+madvise
kernel/mm/transparent_hugepage/khugepaged/defrag=1
EOF
sudo systemctl enable sysfsutils.service
sudo systemctl restart sysfsutils.service

# Tune Linux kernel settings in a persistent way
cat <<EOF | sudo tee /etc/sysctl.d/99-vm-tuning.conf
# set swappiness to 1 to use swapping as a last resort
vm.swappiness=1
# set max_map_count to allow large memory-mapped files
vm.max_map_count=262144
# set aio-max-nr to allow large asynchronous I/O, required by some docker containers
fs.aio-max-nr=1048576
# set inotify limits to allow large number of files to be watched
fs.inotify.max_user_instances=1024
fs.inotify.max_user_watches=1048576
# allow async-profiler to profile non-root processes
# https://github.com/jvm-profiling-tools/async-profiler#basic-usage
# non-root process requires setting two runtime variables
kernel.perf_event_paranoid=1
kernel.kptr_restrict=0
# https://github.com/jvm-profiling-tools/async-profiler#restrictionslimitations
kernel.perf_event_max_stack=1024
# Profiler allocates 8kB perf_event buffer for each thread of the target process.
# Make sure value is large enough (more than 8 * threads)
kernel.perf_event_mlock_kb=2048
EOF
sudo sysctl -p /etc/sysctl.d/99-vm-tuning.conf

# Configure default number of open files limits for systemd in a persistent way
sudo mkdir -p /etc/systemd/system.conf.d/
cat <<EOF | sudo tee /etc/systemd/system.conf.d/99-limits.conf
[Manager]
DefaultLimitNOFILE=65536:524288
EOF

# Configure number of open files limits for the default user in a persistent way
cat <<EOF | sudo tee /etc/security/limits.d/99-limits.conf
*               soft    nofile          65536
*               hard    nofile          524288
EOF

# Reload systemd to apply the changes to systemd without rebooting
sudo systemctl daemon-reload

# Limit Docker logging to 10MB and 3 files to avoid filling up the disk
cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3"
  }
}
EOF

# Suppress annoying Perl locale warnings which shows up with apt-get when using non-English locales
echo 'Defaults env_keep += "PERL_BADLANG"' | sudo tee /etc/sudoers.d/perl-locale
sudo chmod 440 /etc/sudoers.d/perl-locale
echo "PERL_BADLANG=0" | sudo tee -a /etc/environment
export PERL_BADLANG=0

# Restart Docker to apply the changes
sudo systemctl restart docker

# Exit the SSH session
exit
----

[[reconnect-validation]]
==== Reconnect and Run the Validation

After setting up the VM:

1. Reconnect to the VM:
+
[source,shell]
----
gcloud compute ssh pulsar-release-validation
----

2. Start a tmux session (allows reconnecting if connection drops):
+
[source,shell]
----
tmux
----
+
If you are new to tmux, you can read link:https://www.redhat.com/en/blog/introduction-tmux-linux[this article] for a quick start. For the key bindings, this link:https://tmuxcheatsheet.com/[cheat sheet] is useful.
+
If the connection is lost, you can reconnect to the tmux session with:
+
[source,shell]
----
tmux attach
----

3. Clone or update the repository
+
[source,shell]
----
# Clone the repository if it doesn't exist, otherwise pull the latest changes
[ ! -d pulsar-release-validation ] && git clone https://github.com/lhotari/pulsar-release-validation && cd pulsar-release-validation || cd pulsar-release-validation && git pull origin master
----

4. Run the validation script:
+
[source,shell]
----
# Run the validation script
./scripts/validate_pulsar_release_in_docker.sh 4.0.3 2 | tee validate_pulsar_release_`date +%Y-%m-%d_%H-%M-%S`.log
----

[[snapshot-management]]
=== GCP VM Disk Snapshot Management

[[create-snapshot]]
==== Creating a Disk Snapshot on GCP

After validating a release, you can create a snapshot of the VM's disk to retain the configuration and Maven cache:

[source,shell]
----
# stop the VM before creating a snapshot
gcloud compute instances stop pulsar-release-validation

# lookup the boot disk name for the VM
BOOT_DISK_NAME=$(gcloud compute instances describe pulsar-release-validation --format="value(disks[0].source.basename())")
echo "Boot disk name: $BOOT_DISK_NAME"

# Create a snapshot of the VM's disk
gcloud compute disks snapshot $BOOT_DISK_NAME \
  --snapshot-names=pulsar-release-validation-snapshot \
  --description="Snapshot of Pulsar Release Validation VM with Maven cache"
----

[[delete-vm]]
==== Deleting the VM After Creating a Snapshot

Once you've created a snapshot, you can delete the VM to avoid ongoing charges:

[source,shell]
----
gcloud compute instances delete pulsar-release-validation
----

[[vm-from-snapshot]]
==== Creating a New VM from a Snapshot

When you need to validate a new release, you can create a VM from your snapshot:

[source,shell]
----
# First, create a disk from the snapshot
gcloud compute disks create pulsar-release-validation \
  --source-snapshot=pulsar-release-validation-snapshot \
  --size=200GB \
  --type=pd-ssd

# Then, create a VM using this disk
gcloud compute instances create pulsar-release-validation \
  --machine-type=e2-highcpu-8 \
  --disk=name=pulsar-release-validation,boot=yes
----

Now you can directly continue from the "Reconnect and Run the Validation" step.

[[cost-considerations]]
=== GCP Cost Considerations

[NOTE]
====
Updated prices are available at link:https://cloud.google.com/compute/vm-instance-pricing[Google Cloud VM Instance Pricing] and link:https://cloud.google.com/products/calculator[Google Cloud Pricing Calculator].
====

* *VM Costs*: An `e2-highcpu-8` VM costs approximately $145 per month ($0.21 per hour) when running.
* *Disk Cost*: The 200GB pd-ssd disk costs about $34 per month, whether the VM is running or stopped.
* *Disk Snapshot*: Keeping a disk snapshot costs less than $1/month for a 200GB snapshot.
* *Approach Comparison*:
** *Stopped VM*: This will cost about $34 per month for the disk.
** *Without Snapshot*: Each time you need to validate a release, you create a new VM and go through the entire setup process, including downloading and configuring all dependencies.
** *With Snapshot*: You pay a small monthly fee (<$1) to store the snapshot but save significant time and bandwidth when validating new releases, as the VM will already have Docker installed, system tuned, and Maven dependencies cached.

This snapshot approach is particularly beneficial for frequent release validation or for users with limited bandwidth, as it eliminates the need to repeatedly download large Maven dependencies.